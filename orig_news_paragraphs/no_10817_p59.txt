"Theyâ€™ve used the media. They own Hollywood. And Hollywood, whether we like it or not, dictates the culture of the country," she said. " 