You’ve probably heard that it’s always best to focus on the average of polls. The poll results that often get the most attention are outliers  —   they get attention because they’re shocking, not because they’re representative. But we also know that people often ignore that advice. They want to know the details of the newest poll, and how and why it might be different from the last. And to tell you the truth, I do, too. I read the details and methodology of almost every survey that is released. The problem is that it’s a lot harder than it looks. After a splashy poll is released, Twitter is often overflowing with   but misguided analysis.  It’s true: You really are better off looking at the averages. But if you’re going to assess individual polls this election, here’s a guide on how to do it well. When a poll comes out, I start by looking at the topline results  —   Hillary Clinton is plus 3 percentage points, or Donald J. Trump is plus 1, for example. But it’s also worth looking at vote share  —   whether Mrs. Clinton has 47 percent or 40 percent, for instance. In particular, I care about how close the leading candidate is to 50 percent. There’s more uncertainty the further a candidate is from 50 percent and the larger the number of undecided voters. Until a candidate approaches 50 percent, it’s hard to know whether the lead is because of party unity or because the candidate has won over the key voters needed for victory. This is especially true in a reliably red or blue state: A Democrat who has 40 percent of the vote in Arizona still has a lot to prove, even with a lead. He or she hasn’t yet won the   voters who decide the state’s elections. There was a good example of this in the Arizona Senate race. A lot of polls showed John McCain narrowly ahead, or even behind. But once he won the G. O. P. primary,   voters returned to his side. Usually, anything at 46 percent or above is a good indicator of real strength. Less than that, and you have to wonder about undecided voters. ■ It’s also worth looking at whether there’s a difference between registered and likely voters. In a presidential election year, I generally prefer looking at the registered voter numbers  —   with a caveat. That’s because many methods that screen for likely voters are poor and yield noisy data. Registered voter samples are larger, and there aren’t additional questions to add statistical noise. But here’s the caveat: Registered voter polls tend to overrepresent Democrats, so I often focus on the registered voter number and mentally shift it a point or two toward the Republicans. ■ This year, there’s also the question of whether to look at   or   polls. I’m not sold on which is best. This year’s   candidates don’t have the strong bases of a Ross Perot or John Anderson. It’s possible that the polls that name them will overestimate their support it’s possible that the polls that do not name them will underestimate their support. For now, I’m inclined to split the difference. To get a sense of whether a poll is good or bad news for a certain candidate, I usually compare the results of the poll with the polling averages or the last poll conducted by the same pollster. ■ If the poll is very different from the polling average, there’s a good chance it’s an outlier. ■ If the poll shows a big shift from a prior survey, I also wonder whether the previous poll was an outlier. If so, a candidate might appear to rebound simply because he or she was unusually weak in a prior poll. So compare that prior poll with the average of the time, too. ■ It’s also worth looking at whether the candidate has gained or lost vote share. When candidates fall without good reason, I often assume they’re likelier than not to win back their former supporters. I definitely take note when candidates have won more supporters than they’ve won before. If that happens a lot, it’s a real sign of strength. ■ I also look at the various measures of whether Mr. Trump has a ceiling: like a 50 percent "very unfavorable" rating, or 50 percent who say they would be scared of a Trump presidency. I’m not convinced that those measures actually represent a ceiling. But they very well might, and I am curious about whether he’s making progress by those measures. I also care a lot about how the poll was conducted. Polling is hard. A lot of things can go wrong, from question wording to weighting a sample or selecting likely voters. These choices can make a big difference  —   even with the same data. Worse, there are very few pollsters that really provide enough information to know whether they’re making reasonable choices  —   and what the consequences of those decisions might be. As a result, I strongly prefer experienced firms with solid track records. ■ I have serious problems with IVR polls (interactive voice response). These are sometimes called   and they don’t have any means to contact voters with a cellphone. This is a problem for even an experienced firm with a good record. It can be disastrous for firms without much  . ■ I have reservations about online polls. It’s harder to draw a representative sample online because you can’t rely on traditional random sampling. There are skilled and experienced pollsters that do a good job, like YouGov and SurveyMonkey. But there’s a somewhat higher burden for online pollsters to build solid track records, make good hires and publish a detailed methodology. ■ If the poll is from a less established firm, I’ll give a closer look if it has been transparent  —   if the pollsters publish a detailed methodology that gives me a better sense of what I’m getting. You don’t have to analyze every detail of a poll’s methodology, but here are some rules of thumb: ■ A   poll is often a bad poll. You usually need multiple days to call voters back and get a representative sample. ■ Well over half of adults ought to be reached on cellphones in a typical national survey about 40 percent of Americans do not have a landline at all. A poll of registered voters off the voter file can have a somewhat smaller number of   voters, since the poll can typically be weighted by partisanship, and registered and likely voters are less likely to have only a cellphone (they’re older, more affluent and often less mobile than the population as a whole). ■ Pay attention to sample size. If a poll has a small sample  —   less than 800 people or so  —   be aware that sampling error will play a bigger role than usual. The gains from big samples are smaller than you might think, so don’t give a lot of extra credit for a poll that goes much higher than 1, 200. Often, the polls with huge samples are actually just using cheap and problematic sampling methods. If the sample is less than 400, the result should be considered no more than a ballpark estimate. ■ I don’t usually pay much attention to the margin of error because it does not even come close to approximating the actual potential for error in a survey. And pollsters calculate margin of error somewhat differently. In particular, I see a lot of   firms that don’t adjust their margins of error to account for the effect of weighting  —   making their poll seem more confident than it is. The country is deeply divided along demographic lines, which makes it very important to check on the demographic composition of the sample. But this is not so simple to do. ■ Know the polling universe: adults, registered voters and likely voters. This is trickier than you might suppose. Often, a poll of registered or likely voters will report the demographic composition of the overall sample of adults  —   generally a much more diverse group. That doesn’t tell you about the demographic makeup of registered or likely voters. ■ Know the targets. Almost all pollsters adjust their sample to match demographics, like age, race and gender. If a pollster isn’t weighting for something fundamental like that, it’s a big warning sign: See for yourself whether the poll is off by a wide margin on those measures (sometimes, they’re fine). It’s also worth looking at education or party registration. Not every poll weights by those measures, and sometimes that’s a mistake. ■ The exit polls are not the word of God. There are legitimate debates about the electorate. Many of these debates center on differences between estimates based on the census, voter files or exit polls. Most polls are on the end of the spectrum showing a whiter sample, since most polls use voter file and census data. Many times, polls get slammed on social media for being "too white" in comparison with the exit polls, even though they’re near the consensus of more reliable measures. ■ Wrong is wrong. In general, I wouldn’t criticize a poll if the electorate fell somewhere in the debatable range around the exit polls, census and voter file. But if a pollster leaves the debatable range, it’s probably just wrong. It may even indicate colossal failures in weighting or sampling. The poll can be dismissed out of hand on this basis alone. Examples of what I mean: a poll showing that more than 45 percent of the electorate in Pennsylvania will be over age 65 (that’s too high) or that 13 percent of the electorate will be black in North Carolina (that’s too low). ■ I really don’t look at party identification. We have no idea what the "right" partisan breakdown of the electorate really is: It’s an attitude, not a nearly fixed characteristic. Depending on the news or the national political environment,   or   voters can switch in and out of the "unaffiliated" or "independent" column. It is very clear that there are more   Democrats than Republicans in the country, which has been true for about a decade. But I will look at party registration, if it’s available from the voter file. That’s a pretty fixed characteristic: It doesn’t swing with the mood. ■ When I look through the polling crosstabs for subgroups  —   like young people or black voters  —   I compare with old polls, not the exit polls. The two measures are irreconcilable, and a direct comparison introduces a lot of bias. It will generally show, for instance, Democrats doing better among white voters even as they’re not doing better over all. ■ Subgroups are noisy, and that’s fine. I often see a lot of people who dig into the polls and find a crosstab that doesn’t make sense  —   maybe Hispanic voters give Hillary Clinton only 55 percent of the vote. Well, that’s O. K. For one, it matters a lot less than you might think (giving Mrs. Clinton an additional   shift among Hispanic voters will boost her national vote share by just two points). And subsamples are noisy by nature. It might be canceled out by noise elsewhere in the sample, either by chance or for a more structural reason. An underappreciated fact is that polls are usually adjusted for the right number of voters by race, age and gender, but subgroups don’t each have the right demographic composition. There won’t necessarily be the right number of young,   female Hispanics, or old, less educated white men. So a Hispanic subsample could be too old and male, but in exchange, some other part of the electorate is too young and female. ■ A lot of polls conducted off voter registration files will select a certain set of past voters, like people who have voted in the last two elections in addition to those who are newly registered. These decisions can exclude less likely voters, who may vote differently. That’s a problem. I get even more concerned when I see a   screen on top of a sample that was already especially likely to vote. I could go on. But these are the basics of my mental checklist in judging whether an individual poll might maintain the status quo, move the needle or stand as an outlier. And then I go back to the poll averages.
